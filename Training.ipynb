{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertbkts/Bitcoin-Price-Prediction-Using-Twitter-Sentiment-Analysis/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw5vsY-Np8NQ"
      },
      "source": [
        "# Preparation For Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox8KdQCAp-gu"
      },
      "source": [
        "Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4EuRuZtqwNJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LeakyReLU, BatchNormalization, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeV9oDq06h4u",
        "outputId": "94a7be4b-f57d-4c03-9faa-fcb19239737e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCBEkL4JqLmh"
      },
      "source": [
        "Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ggU_kae9rF_S",
        "outputId": "2367e358-c35c-42bf-d1db-a2c22ac03cba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Count_Negatives  Count_Neutrals  Count_Positives     Open    Close\n",
              "0              148             476              403  2855.81  2825.92\n",
              "1              143             396              239  2823.01  2853.38\n",
              "2              118             385              333  2846.27  2841.60\n",
              "3              262             443              279  2841.84  2862.93\n",
              "4              133             381              237  2862.92  2874.99"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ab9eba2-14ca-4b28-bbf6-f2b2cee330f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Count_Negatives</th>\n",
              "      <th>Count_Neutrals</th>\n",
              "      <th>Count_Positives</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>148</td>\n",
              "      <td>476</td>\n",
              "      <td>403</td>\n",
              "      <td>2855.81</td>\n",
              "      <td>2825.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>143</td>\n",
              "      <td>396</td>\n",
              "      <td>239</td>\n",
              "      <td>2823.01</td>\n",
              "      <td>2853.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>118</td>\n",
              "      <td>385</td>\n",
              "      <td>333</td>\n",
              "      <td>2846.27</td>\n",
              "      <td>2841.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>262</td>\n",
              "      <td>443</td>\n",
              "      <td>279</td>\n",
              "      <td>2841.84</td>\n",
              "      <td>2862.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>133</td>\n",
              "      <td>381</td>\n",
              "      <td>237</td>\n",
              "      <td>2862.92</td>\n",
              "      <td>2874.99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ab9eba2-14ca-4b28-bbf6-f2b2cee330f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ab9eba2-14ca-4b28-bbf6-f2b2cee330f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ab9eba2-14ca-4b28-bbf6-f2b2cee330f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dataset = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/TwitterBitcoinTahminSistemi/Dataset/TwitterDataset.csv\" , delimiter=';')\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfzt4oVbs9or"
      },
      "outputs": [],
      "source": [
        "# Extract the value \"Close\" from dataset. It will be the value we will try to predict \n",
        "Y=dataset.Close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CwJD6tUc-yEu",
        "outputId": "07d6eb5c-6372-4a5d-87b8-ec61f689c43d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Count_Negatives  Count_Neutrals  Count_Positives     Open\n",
              "0                  148             476              403  2855.81\n",
              "1                  143             396              239  2823.01\n",
              "2                  118             385              333  2846.27\n",
              "3                  262             443              279  2841.84\n",
              "4                  133             381              237  2862.92\n",
              "...                ...             ...              ...      ...\n",
              "12352              300             335              425  3580.20\n",
              "12353              245             346              387  3578.11\n",
              "12354              186             310              421  3580.26\n",
              "12355              247             305              377  3561.58\n",
              "12356              168             376              355  3569.34\n",
              "\n",
              "[12357 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc942ce2-9ffb-4453-80bf-3322bdb4645e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Count_Negatives</th>\n",
              "      <th>Count_Neutrals</th>\n",
              "      <th>Count_Positives</th>\n",
              "      <th>Open</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>148</td>\n",
              "      <td>476</td>\n",
              "      <td>403</td>\n",
              "      <td>2855.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>143</td>\n",
              "      <td>396</td>\n",
              "      <td>239</td>\n",
              "      <td>2823.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>118</td>\n",
              "      <td>385</td>\n",
              "      <td>333</td>\n",
              "      <td>2846.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>262</td>\n",
              "      <td>443</td>\n",
              "      <td>279</td>\n",
              "      <td>2841.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>133</td>\n",
              "      <td>381</td>\n",
              "      <td>237</td>\n",
              "      <td>2862.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12352</th>\n",
              "      <td>300</td>\n",
              "      <td>335</td>\n",
              "      <td>425</td>\n",
              "      <td>3580.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12353</th>\n",
              "      <td>245</td>\n",
              "      <td>346</td>\n",
              "      <td>387</td>\n",
              "      <td>3578.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12354</th>\n",
              "      <td>186</td>\n",
              "      <td>310</td>\n",
              "      <td>421</td>\n",
              "      <td>3580.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12355</th>\n",
              "      <td>247</td>\n",
              "      <td>305</td>\n",
              "      <td>377</td>\n",
              "      <td>3561.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12356</th>\n",
              "      <td>168</td>\n",
              "      <td>376</td>\n",
              "      <td>355</td>\n",
              "      <td>3569.34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12357 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc942ce2-9ffb-4453-80bf-3322bdb4645e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc942ce2-9ffb-4453-80bf-3322bdb4645e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc942ce2-9ffb-4453-80bf-3322bdb4645e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Remove the value \"Close\" from dataset and assign it to X\n",
        "X=dataset.drop(columns=\"Close\")\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxTOq3tOtLkM",
        "outputId": "bbf56a27-3a82-4d7a-86b6-9012889c2bef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        2825.92\n",
              "1        2853.38\n",
              "2        2841.60\n",
              "3        2862.93\n",
              "4        2874.99\n",
              "          ...   \n",
              "12352    3579.58\n",
              "12353    3580.66\n",
              "12354    3560.70\n",
              "12355    3567.92\n",
              "12356    3573.67\n",
              "Name: Close, Length: 12357, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "Y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test Split"
      ],
      "metadata": {
        "id": "xySWkVxszpoq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5szXi79tQVF"
      },
      "outputs": [],
      "source": [
        "# Splitting data into training and test sets. 80% of the data will be the training set and 20% will be the test set.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Z_01buOYysus",
        "outputId": "4e2676f8-daac-499e-8a0a-2e1590556755"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Count_Negatives  Count_Neutrals  Count_Positives      Open\n",
              "501               180             556              300   4222.30\n",
              "909               159             333              271   4190.61\n",
              "4459              246            1255              839   8492.28\n",
              "570               135             524              459   4295.11\n",
              "6273              226             632              614   9694.31\n",
              "...               ...             ...              ...       ...\n",
              "6528              218            1037              740   8769.31\n",
              "2693              412            1302             1277   9164.96\n",
              "8076              255             716              670   7461.12\n",
              "3829              539            1116              965  14116.64\n",
              "7624              110             339              347   6317.47\n",
              "\n",
              "[9885 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28690607-8a32-4e46-b200-b3800f840ac9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Count_Negatives</th>\n",
              "      <th>Count_Neutrals</th>\n",
              "      <th>Count_Positives</th>\n",
              "      <th>Open</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>180</td>\n",
              "      <td>556</td>\n",
              "      <td>300</td>\n",
              "      <td>4222.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>909</th>\n",
              "      <td>159</td>\n",
              "      <td>333</td>\n",
              "      <td>271</td>\n",
              "      <td>4190.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4459</th>\n",
              "      <td>246</td>\n",
              "      <td>1255</td>\n",
              "      <td>839</td>\n",
              "      <td>8492.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>135</td>\n",
              "      <td>524</td>\n",
              "      <td>459</td>\n",
              "      <td>4295.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6273</th>\n",
              "      <td>226</td>\n",
              "      <td>632</td>\n",
              "      <td>614</td>\n",
              "      <td>9694.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6528</th>\n",
              "      <td>218</td>\n",
              "      <td>1037</td>\n",
              "      <td>740</td>\n",
              "      <td>8769.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2693</th>\n",
              "      <td>412</td>\n",
              "      <td>1302</td>\n",
              "      <td>1277</td>\n",
              "      <td>9164.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8076</th>\n",
              "      <td>255</td>\n",
              "      <td>716</td>\n",
              "      <td>670</td>\n",
              "      <td>7461.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3829</th>\n",
              "      <td>539</td>\n",
              "      <td>1116</td>\n",
              "      <td>965</td>\n",
              "      <td>14116.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7624</th>\n",
              "      <td>110</td>\n",
              "      <td>339</td>\n",
              "      <td>347</td>\n",
              "      <td>6317.47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9885 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28690607-8a32-4e46-b200-b3800f840ac9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-28690607-8a32-4e46-b200-b3800f840ac9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-28690607-8a32-4e46-b200-b3800f840ac9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFMc8WFtvi4_",
        "outputId": "aad2c992-2e70-4b55-8a39-b30188abae96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "501      4227.03\n",
              "909      4276.30\n",
              "4459     8777.94\n",
              "570      4300.00\n",
              "6273     9716.05\n",
              "          ...   \n",
              "6528     8757.40\n",
              "2693     9350.45\n",
              "8076     7390.01\n",
              "3829    14187.42\n",
              "7624     6323.73\n",
              "Name: Close, Length: 9885, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jf6JChmtg4R"
      },
      "outputs": [],
      "source": [
        "# Reshaping Y to 1 dimensional column array\n",
        "Y_train=np.asarray(Y_train).reshape(-1,1)\n",
        "Y_test=np.asarray(Y_test).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u1K68GJvcgJ",
        "outputId": "cfa0ee4b-86ed-4edc-e227-126ef5f05174"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4227.03],\n",
              "       [ 4276.3 ],\n",
              "       [ 8777.94],\n",
              "       ...,\n",
              "       [ 7390.01],\n",
              "       [14187.42],\n",
              "       [ 6323.73]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYNB8BSfvqt9"
      },
      "outputs": [],
      "source": [
        "# Standardization of X\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8RPPiODZ7D2",
        "outputId": "a273e0ca-079e-474f-f95e-bdd460857da3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.bin']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Store scaling parameters for future predictions\n",
        "from joblib import dump, load\n",
        "\n",
        "dump(scaler, 'scaler.bin', compress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building The Model And Training\n",
        "\n"
      ],
      "metadata": {
        "id": "q_2ieow2iIgi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImBKTF5EFCNj"
      },
      "outputs": [],
      "source": [
        "# Create the model and add layers to it\n",
        "model=Sequential()\n",
        "\n",
        "model.add(Dense(1024, input_dim = X_train.shape[1]))\n",
        "model.add(LeakyReLU())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(LeakyReLU())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(LeakyReLU())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(LeakyReLU())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(LeakyReLU())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.01))\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(LeakyReLU())\n",
        "model.add(Dropout(0.05))\n",
        "\n",
        "model.add(Dense(1,activation=\"linear\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SVP6_q2FE5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe79c5b-5cc6-4301-87bb-981dae00eda2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1024)              5120      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 1024)             4096      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,032,961\n",
            "Trainable params: 1,027,841\n",
            "Non-trainable params: 5,120\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Summarize the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(), \n",
        "              loss='mean_absolute_error')"
      ],
      "metadata": {
        "id": "sK39wAmnW7-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtXQInp5FHAl",
        "outputId": "82528b5c-735e-4f71-88ab-2402e8dc7e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "10/10 [==============================] - 5s 229ms/step - loss: 7260.6509 - val_loss: 7304.8438\n",
            "Epoch 2/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 7253.9492 - val_loss: 7294.4829\n",
            "Epoch 3/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 7246.1982 - val_loss: 7280.3926\n",
            "Epoch 4/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 7234.5767 - val_loss: 7266.1001\n",
            "Epoch 5/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 7218.5146 - val_loss: 7251.2769\n",
            "Epoch 6/300\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 7198.5449 - val_loss: 7233.7773\n",
            "Epoch 7/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 7174.7866 - val_loss: 7218.0605\n",
            "Epoch 8/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 7147.1514 - val_loss: 7176.0977\n",
            "Epoch 9/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 7114.2764 - val_loss: 7109.6689\n",
            "Epoch 10/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 7076.8447 - val_loss: 7062.4707\n",
            "Epoch 11/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 7033.4229 - val_loss: 6974.0811\n",
            "Epoch 12/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 6983.5776 - val_loss: 6843.8242\n",
            "Epoch 13/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 6927.3501 - val_loss: 6756.9336\n",
            "Epoch 14/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 6863.0312 - val_loss: 6597.4316\n",
            "Epoch 15/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 6791.3379 - val_loss: 6543.4263\n",
            "Epoch 16/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 6710.1553 - val_loss: 6389.3999\n",
            "Epoch 17/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 6618.3188 - val_loss: 6339.0859\n",
            "Epoch 18/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 6517.8638 - val_loss: 6231.6392\n",
            "Epoch 19/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 6404.6035 - val_loss: 6079.5972\n",
            "Epoch 20/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 6280.8052 - val_loss: 5946.4951\n",
            "Epoch 21/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 6143.8164 - val_loss: 5605.1299\n",
            "Epoch 22/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 5994.5581 - val_loss: 5681.8315\n",
            "Epoch 23/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 5830.9204 - val_loss: 5451.1548\n",
            "Epoch 24/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 5650.9976 - val_loss: 5178.5049\n",
            "Epoch 25/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 5457.0288 - val_loss: 4779.8125\n",
            "Epoch 26/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 5249.6025 - val_loss: 4817.4819\n",
            "Epoch 27/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 5028.1338 - val_loss: 4468.1738\n",
            "Epoch 28/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 4792.1289 - val_loss: 4269.2881\n",
            "Epoch 29/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 4540.2769 - val_loss: 4153.9185\n",
            "Epoch 30/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 4281.0420 - val_loss: 3788.9106\n",
            "Epoch 31/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 4008.0208 - val_loss: 3591.3237\n",
            "Epoch 32/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 3719.3291 - val_loss: 3197.7969\n",
            "Epoch 33/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 3417.1028 - val_loss: 2984.2634\n",
            "Epoch 34/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 3116.2925 - val_loss: 2852.6980\n",
            "Epoch 35/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 2814.2905 - val_loss: 2678.8674\n",
            "Epoch 36/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 2521.6899 - val_loss: 2577.3037\n",
            "Epoch 37/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 2239.6487 - val_loss: 2489.7581\n",
            "Epoch 38/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1954.4241 - val_loss: 2109.2900\n",
            "Epoch 39/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 1691.2634 - val_loss: 2291.7764\n",
            "Epoch 40/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 1416.7135 - val_loss: 1793.8582\n",
            "Epoch 41/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 1164.3743 - val_loss: 1662.5138\n",
            "Epoch 42/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 922.3998 - val_loss: 1475.7939\n",
            "Epoch 43/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 742.8631 - val_loss: 1156.8741\n",
            "Epoch 44/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 605.6641 - val_loss: 1033.2887\n",
            "Epoch 45/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 536.4083 - val_loss: 1250.9178\n",
            "Epoch 46/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 496.8351 - val_loss: 939.0405\n",
            "Epoch 47/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 473.8600 - val_loss: 847.8019\n",
            "Epoch 48/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 474.0077 - val_loss: 933.7831\n",
            "Epoch 49/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 428.5569 - val_loss: 1130.3767\n",
            "Epoch 50/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 446.0575 - val_loss: 1049.5520\n",
            "Epoch 51/300\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 439.1602 - val_loss: 775.1741\n",
            "Epoch 52/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 441.2376 - val_loss: 890.5087\n",
            "Epoch 53/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 417.8712 - val_loss: 807.3691\n",
            "Epoch 54/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 387.8155 - val_loss: 804.2202\n",
            "Epoch 55/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 388.1465 - val_loss: 629.1195\n",
            "Epoch 56/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 369.0681 - val_loss: 668.7055\n",
            "Epoch 57/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 378.2367 - val_loss: 648.6104\n",
            "Epoch 58/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 385.3772 - val_loss: 552.3846\n",
            "Epoch 59/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 387.3250 - val_loss: 453.8927\n",
            "Epoch 60/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 392.2295 - val_loss: 544.7508\n",
            "Epoch 61/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 370.4347 - val_loss: 398.4307\n",
            "Epoch 62/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 382.0275 - val_loss: 510.2363\n",
            "Epoch 63/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 386.0925 - val_loss: 403.7402\n",
            "Epoch 64/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 355.1012 - val_loss: 432.4079\n",
            "Epoch 65/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 356.5814 - val_loss: 293.8213\n",
            "Epoch 66/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 366.6817 - val_loss: 311.6612\n",
            "Epoch 67/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 350.7915 - val_loss: 307.9201\n",
            "Epoch 68/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 356.8528 - val_loss: 277.3139\n",
            "Epoch 69/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 356.7076 - val_loss: 281.0369\n",
            "Epoch 70/300\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 354.5777 - val_loss: 426.7713\n",
            "Epoch 71/300\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 375.5829 - val_loss: 323.4651\n",
            "Epoch 72/300\n",
            "10/10 [==============================] - 2s 196ms/step - loss: 330.1186 - val_loss: 303.9207\n",
            "Epoch 73/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 334.2587 - val_loss: 364.0211\n",
            "Epoch 74/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 344.5218 - val_loss: 273.8224\n",
            "Epoch 75/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 373.7455 - val_loss: 217.2735\n",
            "Epoch 76/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 373.0929 - val_loss: 348.6430\n",
            "Epoch 77/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 353.8374 - val_loss: 391.7792\n",
            "Epoch 78/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 345.7827 - val_loss: 296.2812\n",
            "Epoch 79/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 330.7899 - val_loss: 365.3655\n",
            "Epoch 80/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 314.2509 - val_loss: 270.9239\n",
            "Epoch 81/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 312.1336 - val_loss: 242.9880\n",
            "Epoch 82/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 316.3977 - val_loss: 228.4899\n",
            "Epoch 83/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 311.1089 - val_loss: 268.8748\n",
            "Epoch 84/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 320.3389 - val_loss: 234.0999\n",
            "Epoch 85/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 319.5638 - val_loss: 217.6477\n",
            "Epoch 86/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 334.1969 - val_loss: 223.2590\n",
            "Epoch 87/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 322.7140 - val_loss: 254.5021\n",
            "Epoch 88/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 297.3734 - val_loss: 204.8784\n",
            "Epoch 89/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 316.9416 - val_loss: 265.3305\n",
            "Epoch 90/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 311.2176 - val_loss: 169.5799\n",
            "Epoch 91/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 334.1028 - val_loss: 234.8927\n",
            "Epoch 92/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 321.8455 - val_loss: 307.6821\n",
            "Epoch 93/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 316.2541 - val_loss: 271.9399\n",
            "Epoch 94/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 300.9804 - val_loss: 195.1581\n",
            "Epoch 95/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 295.0750 - val_loss: 192.5610\n",
            "Epoch 96/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 314.0029 - val_loss: 188.8039\n",
            "Epoch 97/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 310.3212 - val_loss: 245.2091\n",
            "Epoch 98/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 304.9043 - val_loss: 226.5770\n",
            "Epoch 99/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 306.9325 - val_loss: 162.2597\n",
            "Epoch 100/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 297.3182 - val_loss: 244.3902\n",
            "Epoch 101/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 323.6368 - val_loss: 409.6103\n",
            "Epoch 102/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 316.0194 - val_loss: 284.9578\n",
            "Epoch 103/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 301.4291 - val_loss: 283.3525\n",
            "Epoch 104/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 315.1681 - val_loss: 250.8115\n",
            "Epoch 105/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 292.2942 - val_loss: 378.5423\n",
            "Epoch 106/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 309.0656 - val_loss: 215.2077\n",
            "Epoch 107/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 291.6742 - val_loss: 181.7370\n",
            "Epoch 108/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 299.7217 - val_loss: 203.0683\n",
            "Epoch 109/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 301.2077 - val_loss: 238.9172\n",
            "Epoch 110/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 286.3293 - val_loss: 152.1604\n",
            "Epoch 111/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 305.6316 - val_loss: 154.2889\n",
            "Epoch 112/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 299.6042 - val_loss: 125.8888\n",
            "Epoch 113/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 301.9449 - val_loss: 156.8791\n",
            "Epoch 114/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 288.2958 - val_loss: 132.6231\n",
            "Epoch 115/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 293.7740 - val_loss: 127.3945\n",
            "Epoch 116/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 281.5490 - val_loss: 178.1547\n",
            "Epoch 117/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 292.9542 - val_loss: 218.4253\n",
            "Epoch 118/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 296.2973 - val_loss: 178.1494\n",
            "Epoch 119/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 287.2755 - val_loss: 157.5582\n",
            "Epoch 120/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 289.1788 - val_loss: 172.9703\n",
            "Epoch 121/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 287.6977 - val_loss: 137.0687\n",
            "Epoch 122/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 294.0536 - val_loss: 158.9464\n",
            "Epoch 123/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 281.3901 - val_loss: 164.6794\n",
            "Epoch 124/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 283.6570 - val_loss: 201.6452\n",
            "Epoch 125/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 302.5571 - val_loss: 190.8447\n",
            "Epoch 126/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 299.7617 - val_loss: 185.3459\n",
            "Epoch 127/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 287.8880 - val_loss: 136.1939\n",
            "Epoch 128/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 282.8425 - val_loss: 179.0847\n",
            "Epoch 129/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 272.2154 - val_loss: 161.5780\n",
            "Epoch 130/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 287.4694 - val_loss: 188.9472\n",
            "Epoch 131/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 281.5327 - val_loss: 127.5056\n",
            "Epoch 132/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 270.6099 - val_loss: 161.8210\n",
            "Epoch 133/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 288.3839 - val_loss: 142.9848\n",
            "Epoch 134/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 289.8254 - val_loss: 224.0882\n",
            "Epoch 135/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 270.8995 - val_loss: 138.4169\n",
            "Epoch 136/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 280.6143 - val_loss: 189.3460\n",
            "Epoch 137/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 273.6969 - val_loss: 109.1364\n",
            "Epoch 138/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 281.6486 - val_loss: 89.6960\n",
            "Epoch 139/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 282.3978 - val_loss: 116.1602\n",
            "Epoch 140/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 280.2256 - val_loss: 124.2704\n",
            "Epoch 141/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 276.1776 - val_loss: 101.3454\n",
            "Epoch 142/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 261.1750 - val_loss: 131.4076\n",
            "Epoch 143/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 275.1281 - val_loss: 169.2066\n",
            "Epoch 144/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 278.8757 - val_loss: 176.9210\n",
            "Epoch 145/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 271.5252 - val_loss: 143.5572\n",
            "Epoch 146/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 272.6531 - val_loss: 106.4866\n",
            "Epoch 147/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 281.6614 - val_loss: 188.5293\n",
            "Epoch 148/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 278.8444 - val_loss: 181.5811\n",
            "Epoch 149/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 276.0901 - val_loss: 115.1665\n",
            "Epoch 150/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 276.5529 - val_loss: 124.1600\n",
            "Epoch 151/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 269.1098 - val_loss: 185.1482\n",
            "Epoch 152/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 284.8599 - val_loss: 97.6222\n",
            "Epoch 153/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 285.8103 - val_loss: 116.8277\n",
            "Epoch 154/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 272.9807 - val_loss: 193.7760\n",
            "Epoch 155/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 273.3272 - val_loss: 231.6730\n",
            "Epoch 156/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 267.6713 - val_loss: 235.8126\n",
            "Epoch 157/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 271.5378 - val_loss: 132.4001\n",
            "Epoch 158/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 274.8910 - val_loss: 239.3286\n",
            "Epoch 159/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 269.0930 - val_loss: 136.1819\n",
            "Epoch 160/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 271.3546 - val_loss: 113.3643\n",
            "Epoch 161/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 260.5770 - val_loss: 185.8929\n",
            "Epoch 162/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 278.2399 - val_loss: 133.0448\n",
            "Epoch 163/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 271.0731 - val_loss: 112.8624\n",
            "Epoch 164/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 265.4216 - val_loss: 199.9101\n",
            "Epoch 165/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 262.4723 - val_loss: 139.2647\n",
            "Epoch 166/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 261.0476 - val_loss: 170.5719\n",
            "Epoch 167/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 259.4503 - val_loss: 89.6114\n",
            "Epoch 168/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 265.1843 - val_loss: 140.9199\n",
            "Epoch 169/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 254.9881 - val_loss: 116.9585\n",
            "Epoch 170/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 252.5480 - val_loss: 102.2107\n",
            "Epoch 171/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 265.9501 - val_loss: 125.0192\n",
            "Epoch 172/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 254.9310 - val_loss: 152.0893\n",
            "Epoch 173/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 255.7994 - val_loss: 124.5809\n",
            "Epoch 174/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 259.1323 - val_loss: 119.9288\n",
            "Epoch 175/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 257.4187 - val_loss: 134.1712\n",
            "Epoch 176/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 257.9616 - val_loss: 124.7974\n",
            "Epoch 177/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 247.2175 - val_loss: 83.9647\n",
            "Epoch 178/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 249.3653 - val_loss: 124.0285\n",
            "Epoch 179/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 261.5708 - val_loss: 110.5920\n",
            "Epoch 180/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 258.6381 - val_loss: 160.1390\n",
            "Epoch 181/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 256.4791 - val_loss: 149.1233\n",
            "Epoch 182/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 257.6030 - val_loss: 76.9537\n",
            "Epoch 183/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 251.1648 - val_loss: 80.7343\n",
            "Epoch 184/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 260.3476 - val_loss: 110.7440\n",
            "Epoch 185/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 261.5952 - val_loss: 97.0902\n",
            "Epoch 186/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 258.5185 - val_loss: 108.1214\n",
            "Epoch 187/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 256.6072 - val_loss: 107.1056\n",
            "Epoch 188/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 263.0028 - val_loss: 96.4305\n",
            "Epoch 189/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 271.5145 - val_loss: 154.4078\n",
            "Epoch 190/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 266.4505 - val_loss: 89.9194\n",
            "Epoch 191/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 267.3406 - val_loss: 132.7148\n",
            "Epoch 192/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 250.6973 - val_loss: 103.3402\n",
            "Epoch 193/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 251.7478 - val_loss: 115.5658\n",
            "Epoch 194/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 262.0414 - val_loss: 109.6245\n",
            "Epoch 195/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 260.3889 - val_loss: 112.7823\n",
            "Epoch 196/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 260.5598 - val_loss: 158.3447\n",
            "Epoch 197/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 247.7886 - val_loss: 104.6717\n",
            "Epoch 198/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 269.1004 - val_loss: 151.7259\n",
            "Epoch 199/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 264.8893 - val_loss: 230.9609\n",
            "Epoch 200/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 256.8562 - val_loss: 154.3352\n",
            "Epoch 201/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 268.4496 - val_loss: 101.7883\n",
            "Epoch 202/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 250.7102 - val_loss: 141.8860\n",
            "Epoch 203/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 246.9650 - val_loss: 109.1146\n",
            "Epoch 204/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 253.4765 - val_loss: 84.6914\n",
            "Epoch 205/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 246.0507 - val_loss: 103.3967\n",
            "Epoch 206/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 241.8888 - val_loss: 116.8279\n",
            "Epoch 207/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 246.0336 - val_loss: 66.9887\n",
            "Epoch 208/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 248.7074 - val_loss: 103.8277\n",
            "Epoch 209/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 252.9229 - val_loss: 99.2678\n",
            "Epoch 210/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 250.4717 - val_loss: 110.3709\n",
            "Epoch 211/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 252.4460 - val_loss: 96.1044\n",
            "Epoch 212/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 246.8282 - val_loss: 103.7320\n",
            "Epoch 213/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 243.0230 - val_loss: 81.5088\n",
            "Epoch 214/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 234.1717 - val_loss: 83.6345\n",
            "Epoch 215/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 264.3475 - val_loss: 92.9250\n",
            "Epoch 216/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 255.9035 - val_loss: 95.7705\n",
            "Epoch 217/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 235.8844 - val_loss: 80.4191\n",
            "Epoch 218/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 241.5284 - val_loss: 101.7248\n",
            "Epoch 219/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 246.5169 - val_loss: 111.1389\n",
            "Epoch 220/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 237.0266 - val_loss: 80.7719\n",
            "Epoch 221/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 244.2254 - val_loss: 103.0729\n",
            "Epoch 222/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 245.1753 - val_loss: 75.7424\n",
            "Epoch 223/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 250.6027 - val_loss: 123.4747\n",
            "Epoch 224/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 261.6140 - val_loss: 73.8276\n",
            "Epoch 225/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 244.5322 - val_loss: 79.4251\n",
            "Epoch 226/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 251.6006 - val_loss: 88.5859\n",
            "Epoch 227/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 250.4527 - val_loss: 111.2497\n",
            "Epoch 228/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 247.0139 - val_loss: 63.9403\n",
            "Epoch 229/300\n",
            "10/10 [==============================] - 2s 197ms/step - loss: 236.4202 - val_loss: 71.3893\n",
            "Epoch 230/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 245.7807 - val_loss: 106.0930\n",
            "Epoch 231/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 260.0525 - val_loss: 119.9481\n",
            "Epoch 232/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 258.8394 - val_loss: 122.7391\n",
            "Epoch 233/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 249.7374 - val_loss: 80.1925\n",
            "Epoch 234/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 244.2293 - val_loss: 114.5546\n",
            "Epoch 235/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 244.9422 - val_loss: 121.3076\n",
            "Epoch 236/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 229.7605 - val_loss: 85.7883\n",
            "Epoch 237/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 243.2480 - val_loss: 102.4339\n",
            "Epoch 238/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 242.3082 - val_loss: 97.9588\n",
            "Epoch 239/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 244.5197 - val_loss: 100.3991\n",
            "Epoch 240/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 234.9881 - val_loss: 74.2467\n",
            "Epoch 241/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 238.2193 - val_loss: 80.8398\n",
            "Epoch 242/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 236.7277 - val_loss: 88.8969\n",
            "Epoch 243/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 244.0222 - val_loss: 77.0177\n",
            "Epoch 244/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 226.8857 - val_loss: 91.2405\n",
            "Epoch 245/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 233.6198 - val_loss: 76.1089\n",
            "Epoch 246/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 248.9253 - val_loss: 74.3679\n",
            "Epoch 247/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 235.2348 - val_loss: 75.5142\n",
            "Epoch 248/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 247.6891 - val_loss: 81.5259\n",
            "Epoch 249/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 231.3701 - val_loss: 81.6116\n",
            "Epoch 250/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 231.2576 - val_loss: 70.2300\n",
            "Epoch 251/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 242.8901 - val_loss: 82.8846\n",
            "Epoch 252/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 245.4704 - val_loss: 128.4078\n",
            "Epoch 253/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 259.3780 - val_loss: 115.3599\n",
            "Epoch 254/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 245.4525 - val_loss: 125.9919\n",
            "Epoch 255/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 226.2189 - val_loss: 89.8016\n",
            "Epoch 256/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 235.5370 - val_loss: 88.5839\n",
            "Epoch 257/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 232.9140 - val_loss: 85.7679\n",
            "Epoch 258/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 232.3572 - val_loss: 134.0189\n",
            "Epoch 259/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 238.9780 - val_loss: 80.5381\n",
            "Epoch 260/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 236.5073 - val_loss: 122.6906\n",
            "Epoch 261/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 236.6855 - val_loss: 74.0807\n",
            "Epoch 262/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 229.5290 - val_loss: 130.2611\n",
            "Epoch 263/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 241.0971 - val_loss: 124.9288\n",
            "Epoch 264/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 226.3393 - val_loss: 89.3171\n",
            "Epoch 265/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 228.1802 - val_loss: 96.5434\n",
            "Epoch 266/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 236.0895 - val_loss: 112.3785\n",
            "Epoch 267/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 230.4300 - val_loss: 87.7174\n",
            "Epoch 268/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 241.9682 - val_loss: 67.3001\n",
            "Epoch 269/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 224.2641 - val_loss: 104.0710\n",
            "Epoch 270/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 233.4141 - val_loss: 79.0747\n",
            "Epoch 271/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 233.4256 - val_loss: 108.7916\n",
            "Epoch 272/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 244.4466 - val_loss: 117.9905\n",
            "Epoch 273/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 255.0491 - val_loss: 74.6746\n",
            "Epoch 274/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 251.2015 - val_loss: 113.3301\n",
            "Epoch 275/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 227.9379 - val_loss: 95.4654\n",
            "Epoch 276/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 235.9970 - val_loss: 87.4923\n",
            "Epoch 277/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 227.1998 - val_loss: 92.4082\n",
            "Epoch 278/300\n",
            "10/10 [==============================] - 2s 198ms/step - loss: 228.4653 - val_loss: 60.6440\n",
            "Epoch 279/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 240.1459 - val_loss: 88.6352\n",
            "Epoch 280/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 227.3235 - val_loss: 122.7398\n",
            "Epoch 281/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 236.0360 - val_loss: 85.4725\n",
            "Epoch 282/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 224.6994 - val_loss: 81.6536\n",
            "Epoch 283/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 233.3570 - val_loss: 113.9439\n",
            "Epoch 284/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 227.3298 - val_loss: 63.9541\n",
            "Epoch 285/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 233.7020 - val_loss: 64.9234\n",
            "Epoch 286/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 235.5294 - val_loss: 120.1752\n",
            "Epoch 287/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 219.9159 - val_loss: 81.8875\n",
            "Epoch 288/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 245.8202 - val_loss: 147.3383\n",
            "Epoch 289/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 261.2331 - val_loss: 116.0362\n",
            "Epoch 290/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 234.4937 - val_loss: 109.0825\n",
            "Epoch 291/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 233.3421 - val_loss: 84.3593\n",
            "Epoch 292/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 237.0715 - val_loss: 122.5015\n",
            "Epoch 293/300\n",
            "10/10 [==============================] - 2s 199ms/step - loss: 239.4349 - val_loss: 59.2192\n",
            "Epoch 294/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 227.3232 - val_loss: 87.8633\n",
            "Epoch 295/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 233.2924 - val_loss: 66.4717\n",
            "Epoch 296/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 238.1024 - val_loss: 116.9145\n",
            "Epoch 297/300\n",
            "10/10 [==============================] - 2s 200ms/step - loss: 232.7645 - val_loss: 85.8037\n",
            "Epoch 298/300\n",
            "10/10 [==============================] - 2s 203ms/step - loss: 231.4376 - val_loss: 89.3816\n",
            "Epoch 299/300\n",
            "10/10 [==============================] - 2s 201ms/step - loss: 244.7905 - val_loss: 98.2590\n",
            "Epoch 300/300\n",
            "10/10 [==============================] - 2s 202ms/step - loss: 238.4384 - val_loss: 97.7934\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "history = model.fit(X_train, Y_train,\n",
        "                    epochs=300, batch_size=1024,\n",
        "                    validation_data=(X_test, Y_test),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPmmU5BBIxLA"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save(\"model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics"
      ],
      "metadata": {
        "id": "1-auKdMaapEX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg_ewrh9F3BH"
      },
      "outputs": [],
      "source": [
        "# Define this function to print mean absolute percentage error and mean absolute error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "def stats(model, X_test, Y_test ):\n",
        "    \n",
        "    y_predicted=model.predict(X_test)\n",
        "    mae = mean_absolute_error(Y_test, y_predicted)\n",
        "    mape=mean_absolute_percentage_error(Y_test, y_predicted)*100\n",
        "\n",
        "    print(\"{:.0f}   : Mean absolute error \\n\".format(mae))\n",
        "    print(\"{:.2f}% : Mean absolute percentage error\\n\".format(mape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qxpWLBZJslz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "2c50e790-82b1-4a12-d020-539c04306765"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98   : Mean absolute error \n",
            "\n",
            "1.47% : Mean absolute percentage error\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Visualizing model's training history and printing statistics\n",
        "plt.plot(history.history['val_loss'],label=\"val_loss\")\n",
        "plt.plot(history.history['loss'],label=\"loss\")\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "stats(model, X_test, Y_test )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}